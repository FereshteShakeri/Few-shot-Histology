
DATA:
  image_size: 84 # Images will be resized to this value
  train_sources: ['crc-tp'] # Which dataset to use
  val_sources: ['lc25000'] # Which dataset to use
  test_sources: ['breakhis'] # Which dataset to use
  train_transforms: ['random_resized_crop', 'random_flip', 'jitter', 'normalize']
  test_transforms: ['resize', 'center_crop', 'normalize'] #Transforms applied to training data
  data_path: '/ssd/dataset/histology/converted/'
  ckpt_path: 'checkpoints'
  shuffle_queue_size: 500

TRAINING:
  seeds: [2021]
  arch: 'resnet18'
  batch_size: 100
  num_workers: 4
  train_freq: 50
  train_iter: 75000
  loss: '_CrossEntropy'
  focal_gamma: 3
  label_smoothing: 0.1

VALIDATION:
  val_batch_size: 1
  val_iter: 250
  val_freq: 1000

TEST:
  test_batch_size: 1
  test_iter: 1000


AUGMENTATIONS:
  beta: 1.0
  cutmix_prob: 1.0
  augmentation: 'none'

OPTIM:
  lr: 0.0001
  momentum: 0.9
  weight_decay: 0.0
  nesterov: True
  gamma: 0.1

VISU:
  visu: False
  max_s_visu: 1
  max_q_visu: 3

EPISODES:
  num_ways: 5  # Set it if you want a fixed # of ways per task
  num_support: 5 # Set it if you want a fixed # of support samples per class
  num_query: 15 # Set it if you want a fixed # of query samples per class
  min_ways: 2 # Minimum # of ways per task')
  max_ways_upper_bound: 10 # Maximum # of ways per task
  max_num_query: 10 # Maximum # of query samples
  max_support_set_size: 100 # Maximum # of support samples
  min_examples_in_class: 0 # Classes that have less samples will be skipped
  max_support_size_contrib_per_class: 10 # Maximum # of support samples per class
  min_log_weight: -0.69314718055994529 # Do not touch, used to randomly sample support set
  max_log_weight: 0.69314718055994529 # Do not touch, used to randomly sample support set
  ignore_bilevel_ontology: False  #Whether or not to use superclass for BiLevel datasets (e.g breakhist)